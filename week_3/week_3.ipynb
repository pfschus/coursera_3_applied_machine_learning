{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Model Evaluation and Selection\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "A big loop: \n",
    "\n",
    "Representation --> Train models --> Evaluation --> Feature and model refinement --> \n",
    "\n",
    "Choose an evaluation method that matches the goal of your analysis. Compute your evaluation metric for multiple different models, then select the model with the 'best' performance for that evaluation metric.\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "Just looking at accuracy may not provide a good picture of performance. For example, if there are datasets with a very small number of \"yes\" and a large number of \"no\" events. This is called an \"imbalanced class.\"\n",
    "\n",
    "Ex: Two classes  \n",
    "* Relevant (R): the positive class\n",
    "* Not_Relevant (NR): the negative class\n",
    "\n",
    "Out of 1000 randomly selected items, on average only one has an R label and 999 have N label.\n",
    "\n",
    "Accuracy is calculated as Number labeled correctly / Total labeled. \n",
    "\n",
    "If you misclassified the one R event as NR, you would get an accuracy of 999/1000 = 99.9%. That is misleading because you missed 100% of the R events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "dataset = load_digits()\n",
    "X, y = dataset.data, dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 64 features and 1797 data points in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 178\n",
      "1 182\n",
      "2 177\n",
      "3 183\n",
      "4 181\n",
      "5 182\n",
      "6 181\n",
      "7 179\n",
      "8 174\n",
      "9 180\n"
     ]
    }
   ],
   "source": [
    "for class_name, class_count in zip(dataset.target_names, np.bincount(dataset.target)):\n",
    "    print(class_name, class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original labels [1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9]\n",
      "New binary labels [1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_binary_imbalanced = y.copy()\n",
    "y_binary_imbalanced[y_binary_imbalanced != 1] = 0\n",
    "\n",
    "print('Original labels', y[1:30])\n",
    "print('New binary labels', y_binary_imbalanced[1:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1615,  182], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_binary_imbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9088888888888889"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state = 0)\n",
    "\n",
    "svm = SVC(kernel='rbf', C=1).fit(X_train, y_train)\n",
    "svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Classifier \n",
    "Create a dummy classifier. They don't even use the data to classifer, they just use a predefined strategy.\n",
    "\n",
    "* most_frequent  \n",
    "* stratified  \n",
    "* uniform  \n",
    "* constant  \n",
    "\n",
    "This provides a *null metric*- a baseline of if you performed classification without any information about the data. It shouldn't be used for classification but can be used as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train,y_train)\n",
    "\n",
    "y_dummy_predictions = dummy_majority.predict(X_test)\n",
    "y_dummy_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9044444444444445"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_majority.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything was labeled '0' but still produces a pretty good accuracy.\n",
    "\n",
    "If your classifier performs about as well as the null classifier, it could be a sign of:  \n",
    "* Ineffective, erroneous or missing features  \n",
    "* Poor choice of kernel or hyperparameter  \n",
    "* Large class imbalance  \n",
    "\n",
    "Try the linear classifier instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(kernel = 'linear', C=1).fit(X_train, y_train)\n",
    "svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a dummy regressor, strategy may be:  \n",
    "* mean\n",
    "* median\n",
    "* quantile\n",
    "* constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Prediction Outcomes, Conflusion Matrix\n",
    "\n",
    "Confusion matrix:\n",
    "\n",
    "* True negative (and predicted negative)\n",
    "* True Positive (and predicted positive)\n",
    "* False positive (predicted positive, but a true negative)\n",
    "* False negative (predicted negative, but a true positive)\n",
    "\n",
    "Organized as:\n",
    "\n",
    "[[TN   FP]  \n",
    " [FN   TP]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done easily in scikit-learn. Try it with a **dummy classifier where everything is classified as the most frequent value** (in this case it is 0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent class (dummy classifier)\n",
      " [[407   0]\n",
      " [ 43   0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# The negative class (0) is most frequent\n",
    "dummy_majority = DummyClassifier(strategy='most_frequent').fit(X_train,y_train)\n",
    "\n",
    "y_majority_predicted = dummy_majority.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, y_majority_predicted)\n",
    "\n",
    "print('Most frequent class (dummy classifier)\\n', confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at this in detail:\n",
    "\n",
    "* TN: 407, identified *correctly* as negative (0)\n",
    "* FP: 0, nothing identified as positive that is actually negative\n",
    "* FN: 43, identified *falsely* as negative (0)-- actually positive (1)\n",
    "* TP: 0, identified *correctly* as positive (1)\n",
    "\n",
    "This makes sense, since nothing can be classified as positive.\n",
    "\n",
    "Now try it with **Random predictions with same class proportion as training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random class-proportional prediction (dummy classifier)\n",
      " [[356  51]\n",
      " [ 37   6]]\n"
     ]
    }
   ],
   "source": [
    "dummy_classprop = DummyClassifier(strategy = 'stratified').fit(X_train, y_train)\n",
    "y_classprop_predicted = dummy_classprop.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, y_classprop_predicted)\n",
    "\n",
    "print('Random class-proportional prediction (dummy classifier)\\n', confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since the predicted values contain positive and negative values, we have samples in each part of the confusion matrix. Move on...\n",
    "\n",
    "**SVC Linear kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support vector machine classifier (linear kernel, C=1)\n",
      " [[402   5]\n",
      " [  5  38]]\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel='linear', C=1).fit(X_train,y_train)\n",
    "svm_predicted = svm.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, svm_predicted)\n",
    "\n",
    "print('Support vector machine classifier (linear kernel, C=1)\\n', confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best result so far.\n",
    "\n",
    "**Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression classifier (default settings)\n",
      " [[401   6]\n",
      " [  6  37]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "lr_predicted = lr.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, lr_predicted)\n",
    "\n",
    "print('Logistic regression classifier (default settings)\\n',confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree classifier (max_depth = 2)\n",
      " [[400   7]\n",
      " [ 17  26]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=2).fit(X_train,y_train)\n",
    "tree_predicted = dt.predict(X_test)\n",
    "confusion = confusion_matrix(y_test,tree_predicted)\n",
    "\n",
    "print('Decision tree classifier (max_depth = 2)\\n',confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other interesting calculations:\n",
    "    \n",
    "Classification error = $(FN+FP)/(FN+FP+TN+TP)$\n",
    "\n",
    "**Recall**, or True Positive Rate. What fraction of all positive instances does the classifier correctly identify as positive? $TP / (TP+FN)$\n",
    "\n",
    "What if it's important to decrease false positive? Look at **precision**, what fraction of positive predictions are true: $TP / (TP+FP)$ \n",
    "\n",
    "False positive rate, a.k.a. specificity: $FP/(TN+FP)$\n",
    "\n",
    "**Tradeoff between precision and recall**\n",
    "\n",
    "* Recall-oriented machine learning tasks:\n",
    "    * Search and information extraction in legal discovery\n",
    "    * Tumor detection\n",
    "    * Often paired with a human expert to filter out false positives\n",
    "* Precision-oriented machine learning tasks:\n",
    "    * Search engine ranking, query suggestion\n",
    "    * Document classification\n",
    "    * Many customer-facing tasks (users remember failurs)\n",
    "    \n",
    "** F1 score** combines precision and recall into a single number.\n",
    "\n",
    "$$ F_1 = 2* (precision*recall)/(precision+recall) = (2 TP)/(2 TP + FN + FP)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9466666666666667\n",
      "0.7878787878787878\n",
      "0.6046511627906976\n",
      "0.6842105263157895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(accuracy_score(y_test, tree_predicted))\n",
    "print(precision_score(y_test, tree_predicted))\n",
    "print(recall_score(y_test, tree_predicted))\n",
    "print(f1_score(y_test, tree_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a summary of all of those metrics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      not 1       0.96      0.98      0.97       407\n",
      "          1       0.79      0.60      0.68        43\n",
      "\n",
      "avg / total       0.94      0.95      0.94       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, tree_predicted, target_names=['not 1','1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Function\n",
    "\n",
    "* Provides information on how confidently the classifier predicts the positive class (large-magnitude positive values) or negative class (large-magnitude negative values.  \n",
    "* Choosing a fixed decision threshold gives a classification rule  \n",
    "* By sweeping the decision threshold through the entire range of possible score values, we get a series of classification outcomes that form a curve"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
