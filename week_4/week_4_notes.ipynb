{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Supervised ML part 2\n",
    "\n",
    "Patricia Schuster  \n",
    "U. Michigan  \n",
    "Jan 2019\n",
    "\n",
    "## Naive Bayes Classifiers\n",
    "\n",
    "* Based on simple probabilistic models of how the data in each class might have been generated.\n",
    "* Assume that all data is independent / uncorrelated. Not always true. For instance, if you're looking at selling price of a house, the square footage is correlated with number of bedrooms. \n",
    "* Well-suited to high-dimensional data\n",
    "* Easy to understand, often useful as a baseline comparison against more sophisticated methods. For example, simply predict that all data will produce the most frequent target value. \n",
    "\n",
    "Three flavors in `scikit-learn`:\n",
    "\n",
    "* Bernoulli: Binary features (useful for classifying text documents)\n",
    "* Multinomial: discrete count-based features (word counts, also well-suited for text documents)\n",
    "* Gaussian: Continuous or real-valued features.\n",
    "    * Statistics computed for each class: mean, standard deviation\n",
    "    * For prediction, the classifier compares the features to each class, and selects the class that best matches the data point.\n",
    "    * Assumes that the data was generated by a simple, class-specific Gaussian distribution. It then picks the class with the highest probability.\n",
    "    * There are no special parameters to control the model's complexity\n",
    "\n",
    "## Random Forests\n",
    "\n",
    "* Ensemble: Combine multiple individual learning models. Each individual model might overfit part of the data; by combining all the models you can average out their performance\n",
    "* An ensemble of trees, not just one tree. Better generalization. Ensemble of trees should be diverse; introduce random variation into the process of building each decision tree.\n",
    "* Randomly select the data that is used to build the tree (bootstrap sample). Bootstrap sampling: For a dataset size N, randomly pick a set of N samples from the dataset *with replacement* (you can pick the same sample twice in one bootstrap set). Final bootstrap set is the same size but might be missing a few samples and have a few duplicates\n",
    "* Tell the tree to split nodes at random features\n",
    "* Key parameters\n",
    "    * `n_estimators` sets the number of trees to use. Should be larger for larger datasets. Default = 10\n",
    "    * `max_features`: Influences diversity of trees. Generally sqrt(number features)\n",
    "    * `max_depth` controls the depth of each true\n",
    "    * `n_jobs`: How many cores to use in parallel to use to train the model (for multi-core computers)\n",
    "    * `random_state`: Set to a fixed number if you want reproducible results\n",
    "    \n",
    "## Gradient-boosted decision trees\n",
    "\n",
    "* Build a series of trees, each of which tries to correct the mistakes of previous trees\n",
    "* Don't need to apply feature normalization\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "(took some handwritten notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
